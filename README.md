# Machine-learning Report
Enron was formed in 1985 by Kenneth Lay after merging Houston Natural Gas and InterNorth. Several years later, when Jeffrey Skilling was hired, he developed a staff of executives that – by the use of accounting loopholes, special purpose entities, and poor financial reporting, thus were able to hide billions of dollars in debt from failed deals and projects. Chief Financial Officer Andrew Fastow and other executives not only misled Enron's board of directors and audit committee on high-risk accounting practices, but also pressured Arthur Andersen to ignore the issues.  This eventually led to the bankruptcy of the Enron Corporation. After the company's collapse happened, more than 6 lacs mail generated by its employees were made public.

**Objective**

The aim of this project is to identify the fraud based on the data available by employing machine learning algorithm. Machine learning is a great tool to use to make predictions by training and testing the data. This work has been organised as:
- Reading and investigatig the dataset
- Feature Selection
- Checking for Outliers
- Adding new features to the existing list
- Checking the accuracy using different algorithms
- Tuning the algorithm for the required precision and recall

## Reading and exploring the dataset: 
The dataset has records of 146 different persons with  21 different features. However, this is not a perfect dataset. There are many entries in the dataset whose records are missing and have been written as NaN. Despite that, we calculated the number of POIs (persons of interest) and total number comes out to be 18. I also looked at their salary side by side.

## Feature Selection
The features I decided to choose for my investigation includes **poi, salary, bonus, total_payments, loan_advances, to_messages, from_messages, from_poi_to_this_person, from_this_person_to_poi, 'deferral_payments', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock'**. Later on the best feature feature was selected. For GaussianNB and DecisionTreeClassfier, 6 features were found to be most important and they are **'poi', 'shared_receipt_with_poi', 'loan_advances', 'from_poi_to_this_person', 'to_messages', 'from_this_person_to_poi_ratio'**. Their scores are:
- ('shared_receipt_with_poi', 7.5903169251762979),
 - ('from_poi_to_this_person', 4.7396631249076204),
 - ('loan_advances', 2.5578794453272402),
 - ('from_this_person_to_poi', 2.1752399005622309),
 - ('to_messages', 1.6347475070474591)
 
 Similarly for RandomForestClasifier four features are found to be most important. Also, one of the feature that we created is part of the important feature to be included for analysis. The graphs are enclosed separetely. 'kb.py' file contains the code for selecting the best features. Scaling is needed as the dataset obtained after splitting the original dataset is of different dimensions. Thus transforming features by scaling brings feature to a same range.

## Checking for outliers
- A salary versus bonus plot is created and we see that an outlier exists. On investigation, it was found to be 'TOTAL' value actually which has crept in the dataset due to human error. Removing this and replotting shows us that majority of the employees have a salary around 5 million US dollar but few have more who turn out to be the owners and CEO of Enron.
- In the second scatter plot, we check the relationship between the saalry and loan_advance which is the loan one takes from a bank. Surprisingly, there are people who have salary in excess of 100 million and yet have high loans to repay. For the dataset these are outliers but it also exposes that unethial practices were going on in conjuction with the banks. On searching, it turned out to be the CEO of the company, **LAY KENNETH L**.
- Third plot is in sync with second one, where we see the relationship between loan and total amount. **LAVORATO JOHN J**, **BHATNAGAR SANJAY**, **FREVERT MARK A** are the three pople who are identified as outliers here. These outliers are removed and cleaned dataset is created as well as the plots showing the beahvior.

## Adding new features to the existing list
Important people in the company also known as POIs were the one mostly responsible for the collapse of the organization. The email data showed us that. Since a number of mails were sent and received, we wanted to find out who were the most frequent mail writers ad to whom. We look at it from the propects of POIs and so we add two new features: **from_this_person_to_poi_ratio** and **from_poi_to_this_person_ratio**. The list is then appended to include these new features.

## Checking the accuracy using different Algorithms
I used three algorithms here: **GaussianNB**, **DecisionTreeClassifier**, and **RandomForestClassifier**. For all the three cases accuracy was checked and for decision tree, it was found to be 0.902, 0.878 for GaussianNB, and 0.878 for Random Forest.

## Tuning the algorithm for the required precision and recall
Tuning is the process of adjusting the performance of the algorithm. By deafult, algorithms run but they can be modified according to the situation at hand to optimize the performance. On tuning, it was seen that GaussianNB is not the best algorithm for this case as there are no such parameters to tune. For me the algorithm that turned to be the best was DecisionTree. By changing the two parameters **min_samples_split=4, min_samples_leaf=2**, the desired precision and recall was obtained and it came to be **0.5** for precision as well as recall. For GaussianNB, precision obtained is **.5** and recall **0.6**. For RandomForestClassifier, the values are **0.5 and 0.4** respectively. 
 
 ## Validation
Cross-validation is a technique for evaluating ML models by training several ML models on subsets of the available input data and evaluating them on the complementary subset of the data. Use cross-validation to detect overfitting, ie, failing to generalize a pattern. The purpose is to use trained data make reliable predictions which are then validated by the test data. A classical problem that arises in such case is of overfitting. It might happen that our data has been trained very well but it fails to predict on the test dataset. One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique
 ## Conclusion
The precision can be interpreted as the likelihood that a person who is identified as a POI is actually a true POI; the fact that this is 0.6 on average means that using this identifier to flag POI’s would result in 60% of the positive flags being false alarms. Recall measures how likely it is that identifier will flag a POI in the test set. 60% of the time it would catch that person, and 40% of the time it wouldn’t.
